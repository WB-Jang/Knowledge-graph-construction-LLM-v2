# Legal Knowledge Graph v2ğŸ›ï¸
## GPU & Memgraph & Llama-cpp Edition

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/WB-Jang/Knowledge-graph-construction-LLM-v2/blob/main/knowledge_graph_colab.ipynb)

í•œêµ­ì–´ ë²•ë¥  ë¬¸ì„œë¥¼ ì˜¤í”ˆì†ŒìŠ¤ LLMê³¼ Memgraphë¥¼ í™œìš©í•˜ì—¬ ì§€ì‹ê·¸ë˜í”„ë¡œ ë³€í™˜í•˜ëŠ” í”„ë¡œì íŠ¸

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

- **GPU ê°€ì† ì§€ì›** (NVIDIA CUDA 12.1)
- **Memgraph ê·¸ë˜í”„ ë°ì´í„°ë² ì´ìŠ¤**
- **ì™¸ë¶€ llama-cpp API ì—°ë™**
- **2ë‹¨ê³„ ì¶”ì¶œ ë°©ì‹**
  - Step 1: ì¡°í•­ë³„ êµ¬ì¡°í™” ë° ê°œì²´ ì¶”ì¶œ
  - Step 2: ê´€ê³„ ì •ì˜ (Graph Triplets)
- **LangChain & LangGraph ì›Œí¬í”Œë¡œìš°**

## ğŸš€ ì‹œì‘í•˜ê¸°

### ğŸŒ Google Colabì—ì„œ ë¹ ë¥´ê²Œ ì‹œì‘í•˜ê¸° (ê¶Œì¥)

Dockerë‚˜ ë¡œì»¬ í™˜ê²½ ì„¤ì • ì—†ì´ Google Colabì—ì„œ ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/WB-Jang/Knowledge-graph-construction-LLM-v2/blob/main/knowledge_graph_colab.ipynb)

**Colab ë…¸íŠ¸ë¶ íŠ¹ì§•:**
- âœ… ë¬´ë£Œ GPU (T4) ì‚¬ìš© ê°€ëŠ¥
- âœ… Google Gemini API ì—°ë™ (ë¬´ë£Œ tier)
- âœ… PDF íŒŒì¼ ì—…ë¡œë“œ ë° ì²˜ë¦¬
- âœ… ê²°ê³¼ ì‹œê°í™” ë° ë‹¤ìš´ë¡œë“œ
- âœ… í™˜ê²½ ì„¤ì • ë¶ˆí•„ìš”

**ì‚¬ìš© ë°©ë²•:**
1. ìœ„ ë°°ì§€ë¥¼ í´ë¦­í•˜ì—¬ Colab ë…¸íŠ¸ë¶ ì—´ê¸°
2. Google Gemini API í‚¤ ë°œê¸‰ ([API í‚¤ ë°›ê¸°](https://makersuite.google.com/app/apikey))
3. ë…¸íŠ¸ë¶ì˜ ì…€ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
4. ìƒ˜í”Œ ë²•ë¥  í…ìŠ¤íŠ¸ ë˜ëŠ” PDF íŒŒì¼ ì²˜ë¦¬

> ğŸ’¡ **ì°¸ê³ :** Colab í™˜ê²½ì—ì„œëŠ” Memgraphë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë©”ëª¨ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬í•˜ë©°, í…ŒìŠ¤íŠ¸ ëª©ì ìœ¼ë¡œ ì²˜ìŒ 3ê°œ ì¡°í•­ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

---

### ğŸ³ ë¡œì»¬ í™˜ê²½ ë˜ëŠ” Dockerì—ì„œ ì „ì²´ ê¸°ëŠ¥ ì‚¬ìš©í•˜ê¸°

ì „ì²´ ê¸°ëŠ¥(Memgraph, ë¬´ì œí•œ ì¡°í•­ ì²˜ë¦¬ ë“±)ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë¡œì»¬ í™˜ê²½ì´ë‚˜ Dockerë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

### ì „ì œ ì¡°ê±´

1. **NVIDIA GPU & Docker GPU ì§€ì›**
   ```bash
   # NVIDIA Docker ì„¤ì¹˜
   distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
   curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
   curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
     sudo tee /etc/apt/sources.list.d/nvidia-docker.list
   
   sudo apt-get update && sudo apt-get install -y nvidia-docker2
   sudo systemctl restart docker
   ```

2. **llama-cpp-python ì„œë²„ (ì™¸ë¶€ì—ì„œ ì‹¤í–‰)**
   ```bash
   # llama-cpp-python ì„¤ì¹˜
   CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python[server]
   
   # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì˜ˆì‹œ)
   huggingface-cli download beomi/Llama-3-Open-Ko-8B-gguf
   
   # ì„œë²„ ì‹¤í–‰
   python -m llama_cpp.server \
     --model models/llama-3-open-ko-8b.Q4_K_M.gguf \
     --host 0.0.0.0 \
     --port 8000 \
     --n_gpu_layers 35 \
     --n_ctx 4096
   ```

### 1.  í™˜ê²½ ì„¤ì •

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone <your-repo>
cd legal-knowledge-graph

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# . env íŒŒì¼ ìˆ˜ì •
```

**.env ì„¤ì • ì˜ˆì‹œ:**
```bash
LLAMA_CPP_API_URL=http://host.docker.internal:8000
LLM_MODEL_NAME=llama-3-korean-8b
LLM_TEMPERATURE=0.0
LLM_MAX_TOKENS=2048

MEMGRAPH_HOST=memgraph
MEMGRAPH_PORT=7687
```

### 2. Docker Composeë¡œ ì‹¤í–‰

```bash
# GPU í™•ì¸
nvidia-smi

# ì»¨í…Œì´ë„ˆ ë¹Œë“œ ë° ì‹¤í–‰
docker-compose up -d --build

# ë¡œê·¸ í™•ì¸
docker-compose logs -f legal-kg

# GPU ì‚¬ìš© í™•ì¸
docker exec legal-knowledge-graph check-gpu
```

### 3. VSCode Dev Container ì‚¬ìš©

1. VSCodeì—ì„œ í”„ë¡œì íŠ¸ ì—´ê¸°
2. `Ctrl+Shift+P` â†’ "Dev Containers: Reopen in Container"
3. ìë™ìœ¼ë¡œ GPU í™˜ê²½ êµ¬ì„±

### 4. ì‹¤í–‰

#### ì˜ˆì œ ì½”ë“œë¡œ ì‹¤í–‰ (ê¸°ë³¸)
```bash
# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ
poetry run python src/main.py
```

#### PDF íŒŒì¼ë¡œ ì‹¤í–‰
```bash
# 1. PDF íŒŒì¼ì„ data/pdfs/ ë””ë ‰í† ë¦¬ì— ë³µì‚¬
cp /path/to/your/ë²•ë¥ ë¬¸ì„œ.pdf data/pdfs/

# 2. PDF ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
poetry run python src/process_pdf.py
```

**PDF ì²˜ë¦¬ ê³¼ì •:**
1. `data/pdfs/` ë””ë ‰í† ë¦¬ì˜ PDF íŒŒì¼ ëª©ë¡ì´ í‘œì‹œë©ë‹ˆë‹¤
2. ì²˜ë¦¬í•  íŒŒì¼ ë²ˆí˜¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤
3. PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤
4. ë²•ë¥  ì¡°í•­ì„ ë¶„ì„í•˜ê³  ì§€ì‹ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
5. Memgraphì— ì €ì¥í• ì§€ ì„ íƒí•©ë‹ˆë‹¤

**ì°¸ê³ :** PDF íŒŒì¼ì€ í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ íŒŒì¼ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ìŠ¤ìº”ëœ ì´ë¯¸ì§€ PDFëŠ” í˜„ì¬ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

## ğŸ“Š Memgraph Lab

- **URL**: http://localhost:3000
- **Bolt**:  bolt://localhost:7687

### ì¿¼ë¦¬ ì˜ˆì‹œ

```cypher
// ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ
MATCH (d:Document) RETURN d;

// íŠ¹ì • ì¡°í•­ ì¡°íšŒ
MATCH (a:Article {number: "ì œ1ì¡°"}) RETURN a;

// ì¡°í•­ ê°„ ê´€ê³„ ì‹œê°í™”
MATCH (a: Article)-[:HAS_RELATION]->(r)->(e:Entity)
RETURN a, r, e LIMIT 50;

// ê°€ì¥ ë§ì´ ì°¸ì¡°ë˜ëŠ” ê°œì²´
MATCH (e:Entity)<-[r: RELATION]-()
RETURN e. name, count(r) as refs
ORDER BY refs DESC LIMIT 10;
```

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ llm/                # LLM í´ë¼ì´ì–¸íŠ¸
â”‚   â”‚   â”œâ”€â”€ llama_client.py
â”‚   â”‚   â””â”€â”€ gemini_client.py
â”‚   â”œâ”€â”€ chains/             # LangChain ì²´ì¸
â”‚   â”œâ”€â”€ graphs/             # LangGraph ì›Œí¬í”Œë¡œìš°
â”‚   â”œâ”€â”€ database/           # Memgraph í´ë¼ì´ì–¸íŠ¸
â”‚   â”œâ”€â”€ models/             # Pydantic ìŠ¤í‚¤ë§ˆ
â”‚   â”œâ”€â”€ utils/              # ìœ í‹¸ë¦¬í‹°
â”‚   â”‚   â”œâ”€â”€ text_processor.py
â”‚   â”‚   â””â”€â”€ pdf_processor.py  # PDF ì²˜ë¦¬
â”‚   â”œâ”€â”€ main.py             # ì˜ˆì œ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ process_pdf.py      # PDF ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pdfs/               # PDF íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
â”œâ”€â”€ tests/                  # í…ŒìŠ¤íŠ¸ íŒŒì¼
â”œâ”€â”€ models/                 # ë¡œì»¬ LLM ëª¨ë¸ (ë§ˆìš´íŠ¸)
â””â”€â”€ Dockerfile              # GPU Docker ì„¤ì •
```

## ğŸ”§ ê°œë°œ

```bash
# ì˜ì¡´ì„± ì¶”ê°€
poetry add package-name

# GPU í™•ì¸
poetry run python -c "import torch; print(torch. cuda.is_available())"

# í…ŒìŠ¤íŠ¸
poetry run pytest

# ì½”ë“œ í¬ë§¤íŒ…
poetry run black src/
poetry run isort src/
```

## ğŸ¤– ì§€ì›í•˜ëŠ” LLM

ì™¸ë¶€ llama-cpp ì„œë²„ë¥¼ í†µí•´ ë‹¤ìŒ ëª¨ë¸ë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **í•œêµ­ì–´ ëª¨ë¸**
  - beomi/Llama-3-Open-Ko-8B
  - yanolja/EEVE-Korean-10. 8B
  - maywell/EXAONE-3.0-7. 8B-Instruct

- **ë‹¤êµ­ì–´ ëª¨ë¸**
  - meta-llama/Llama-3.1-8B-Instruct
  - mistralai/Mistral-7B-Instruct-v0.3

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

```bash
# GPU ë©”ëª¨ë¦¬ ìµœì í™”
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# Memgraph ë©”ëª¨ë¦¬ ì„¤ì • (docker-compose. yml)
command: ["--memory-limit=8192"]
```

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### GPU ì¸ì‹ ì•ˆë¨
```bash
# NVIDIA ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# Docker GPU ëŸ°íƒ€ì„ í™•ì¸
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

### llama-cpp API ì—°ê²° ì‹¤íŒ¨
```bash
# API ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:8000/v1/models

# Docker ë‚´ë¶€ì—ì„œ í˜¸ìŠ¤íŠ¸ ì ‘ê·¼
# . envì—ì„œ LLAMA_CPP_API_URL=http://host.docker.internal:8000
```

### Memgraph ì—°ê²° ì‹¤íŒ¨
```bash
# Memgraph ìƒíƒœ í™•ì¸
docker-compose ps memgraph

# ë¡œê·¸ í™•ì¸
docker-compose logs memgraph
```

## ğŸ¤ ê¸°ì—¬

ì´ìŠˆì™€ PRì„ í™˜ì˜í•©ë‹ˆë‹¤! 

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License
```
