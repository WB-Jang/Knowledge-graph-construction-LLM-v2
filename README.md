# Legal Knowledge Graph v2ğŸ›ï¸
## GPU & Memgraph & Llama-cpp Edition

í•œêµ­ì–´ ë²•ë¥  ë¬¸ì„œë¥¼ ì˜¤í”ˆì†ŒìŠ¤ LLMê³¼ Memgraphë¥¼ í™œìš©í•˜ì—¬ ì§€ì‹ê·¸ë˜í”„ë¡œ ë³€í™˜í•˜ëŠ” í”„ë¡œì íŠ¸

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

- **GPU ê°€ì† ì§€ì›** (NVIDIA CUDA 12.1)
- **Memgraph ê·¸ë˜í”„ ë°ì´í„°ë² ì´ìŠ¤**
- **ì™¸ë¶€ llama-cpp API ì—°ë™**
- **2ë‹¨ê³„ ì¶”ì¶œ ë°©ì‹**
  - Step 1: ì¡°í•­ë³„ êµ¬ì¡°í™” ë° ê°œì²´ ì¶”ì¶œ
  - Step 2: ê´€ê³„ ì •ì˜ (Graph Triplets)
- **LangChain & LangGraph ì›Œí¬í”Œë¡œìš°**

## ğŸš€ ì‹œì‘í•˜ê¸°

### ì „ì œ ì¡°ê±´

1. **NVIDIA GPU & Docker GPU ì§€ì›**
   ```bash
   # NVIDIA Docker ì„¤ì¹˜
   distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
   curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
   curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
     sudo tee /etc/apt/sources.list.d/nvidia-docker.list
   
   sudo apt-get update && sudo apt-get install -y nvidia-docker2
   sudo systemctl restart docker
   ```

2. **llama-cpp-python ì„œë²„ (ì™¸ë¶€ì—ì„œ ì‹¤í–‰)**
   ```bash
   # llama-cpp-python ì„¤ì¹˜
   CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python[server]
   
   # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì˜ˆì‹œ)
   huggingface-cli download beomi/Llama-3-Open-Ko-8B-gguf
   
   # ì„œë²„ ì‹¤í–‰
   python -m llama_cpp.server \
     --model models/llama-3-open-ko-8b.Q4_K_M.gguf \
     --host 0.0.0.0 \
     --port 8000 \
     --n_gpu_layers 35 \
     --n_ctx 4096
   ```

### 1.  í™˜ê²½ ì„¤ì •

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone <your-repo>
cd legal-knowledge-graph

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# . env íŒŒì¼ ìˆ˜ì •
```

**.env ì„¤ì • ì˜ˆì‹œ:**
```bash
LLAMA_CPP_API_URL=http://host.docker.internal:8000
LLM_MODEL_NAME=llama-3-korean-8b
LLM_TEMPERATURE=0.0
LLM_MAX_TOKENS=2048

MEMGRAPH_HOST=memgraph
MEMGRAPH_PORT=7687
```

### 2. Docker Composeë¡œ ì‹¤í–‰

```bash
# GPU í™•ì¸
nvidia-smi

# ì»¨í…Œì´ë„ˆ ë¹Œë“œ ë° ì‹¤í–‰
docker-compose up -d --build

# ë¡œê·¸ í™•ì¸
docker-compose logs -f legal-kg

# GPU ì‚¬ìš© í™•ì¸
docker exec legal-knowledge-graph check-gpu
```

### 3. VSCode Dev Container ì‚¬ìš©

1. VSCodeì—ì„œ í”„ë¡œì íŠ¸ ì—´ê¸°
2. `Ctrl+Shift+P` â†’ "Dev Containers: Reopen in Container"
3. ìë™ìœ¼ë¡œ GPU í™˜ê²½ êµ¬ì„±

### 4. ì‹¤í–‰

```bash
# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ
poetry run python src/main.py
```

## ğŸ“Š Memgraph Lab

- **URL**: http://localhost:3000
- **Bolt**:  bolt://localhost:7687

### ì¿¼ë¦¬ ì˜ˆì‹œ

```cypher
// ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ
MATCH (d:Document) RETURN d;

// íŠ¹ì • ì¡°í•­ ì¡°íšŒ
MATCH (a:Article {number: "ì œ1ì¡°"}) RETURN a;

// ì¡°í•­ ê°„ ê´€ê³„ ì‹œê°í™”
MATCH (a: Article)-[:HAS_RELATION]->(r)->(e:Entity)
RETURN a, r, e LIMIT 50;

// ê°€ì¥ ë§ì´ ì°¸ì¡°ë˜ëŠ” ê°œì²´
MATCH (e:Entity)<-[r: RELATION]-()
RETURN e. name, count(r) as refs
ORDER BY refs DESC LIMIT 10;
```

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ llm/                # LLM í´ë¼ì´ì–¸íŠ¸
â”‚   â”‚   â””â”€â”€ llama_client.py
â”‚   â”œâ”€â”€ chains/             # LangChain ì²´ì¸
â”‚   â”œâ”€â”€ graphs/             # LangGraph ì›Œí¬í”Œë¡œìš°
â”‚   â”œâ”€â”€ database/           # Memgraph í´ë¼ì´ì–¸íŠ¸
â”‚   â”œâ”€â”€ models/             # Pydantic ìŠ¤í‚¤ë§ˆ
â”‚   â””â”€â”€ utils/              # ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ models/                 # ë¡œì»¬ LLM ëª¨ë¸ (ë§ˆìš´íŠ¸)
â”œâ”€â”€ data/                   # ë°ì´í„° íŒŒì¼
â””â”€â”€ Dockerfile              # GPU Docker ì„¤ì •
```

## ğŸ”§ ê°œë°œ

```bash
# ì˜ì¡´ì„± ì¶”ê°€
poetry add package-name

# GPU í™•ì¸
poetry run python -c "import torch; print(torch. cuda.is_available())"

# í…ŒìŠ¤íŠ¸
poetry run pytest

# ì½”ë“œ í¬ë§¤íŒ…
poetry run black src/
poetry run isort src/
```

## ğŸ¤– ì§€ì›í•˜ëŠ” LLM

ì™¸ë¶€ llama-cpp ì„œë²„ë¥¼ í†µí•´ ë‹¤ìŒ ëª¨ë¸ë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **í•œêµ­ì–´ ëª¨ë¸**
  - beomi/Llama-3-Open-Ko-8B
  - yanolja/EEVE-Korean-10. 8B
  - maywell/EXAONE-3.0-7. 8B-Instruct

- **ë‹¤êµ­ì–´ ëª¨ë¸**
  - meta-llama/Llama-3.1-8B-Instruct
  - mistralai/Mistral-7B-Instruct-v0.3

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

```bash
# GPU ë©”ëª¨ë¦¬ ìµœì í™”
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# Memgraph ë©”ëª¨ë¦¬ ì„¤ì • (docker-compose. yml)
command: ["--memory-limit=8192"]
```

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### GPU ì¸ì‹ ì•ˆë¨
```bash
# NVIDIA ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# Docker GPU ëŸ°íƒ€ì„ í™•ì¸
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

### llama-cpp API ì—°ê²° ì‹¤íŒ¨
```bash
# API ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:8000/v1/models

# Docker ë‚´ë¶€ì—ì„œ í˜¸ìŠ¤íŠ¸ ì ‘ê·¼
# . envì—ì„œ LLAMA_CPP_API_URL=http://host.docker.internal:8000
```

### Memgraph ì—°ê²° ì‹¤íŒ¨
```bash
# Memgraph ìƒíƒœ í™•ì¸
docker-compose ps memgraph

# ë¡œê·¸ í™•ì¸
docker-compose logs memgraph
```

## ğŸ¤ ê¸°ì—¬

ì´ìŠˆì™€ PRì„ í™˜ì˜í•©ë‹ˆë‹¤! 

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License
```
