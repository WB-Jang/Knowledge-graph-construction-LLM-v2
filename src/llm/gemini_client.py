import os
from typing import Optional, Dict, Any, List
from langchain_core.language_models. llms import LLM
from langchain_core.callbacks. manager import CallbackManagerForLLMRun
from langchain_google_genai import ChatGoogleGenerativeAI
from pydantic import Field
import google.generativeai as genai


class GeminiClient:
    """Google Gemini API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        model_name: str = None,
        temperature: float = None,
        max_tokens: int = None
    ):
        self.api_key = api_key or os.getenv("GOOGLE_API_KEY")
        self.model_name = model_name or os.getenv("GEMINI_MODEL", "gemini-1.5-flash")
        self.temperature = temperature if temperature is not None else float(os.getenv("LLM_TEMPERATURE", "0.0"))
        self.max_tokens = max_tokens or int(os.getenv("LLM_MAX_TOKENS", "2048"))
        
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # Gemini API ì„¤ì •
        genai.configure(api_key=self.api_key)
        
        # LangChain Gemini í´ë¼ì´ì–¸íŠ¸
        self.llm = ChatGoogleGenerativeAI(
            model=self.model_name,
            google_api_key=self.api_key,
            temperature=self.temperature,
            max_output_tokens=self.max_tokens,
            convert_system_message_to_human=True  # system ë©”ì‹œì§€ë¥¼ userë¡œ ë³€í™˜
        )
    
    def invoke(self, prompt: str, **kwargs) -> str:
        """ë™ê¸° í˜¸ì¶œ"""
        return self.llm. invoke(prompt, **kwargs).content
    
    async def ainvoke(self, prompt: str, **kwargs) -> str:
        """ë¹„ë™ê¸° í˜¸ì¶œ"""
        result = await self.llm.ainvoke(prompt, **kwargs)
        return result.content
    
    def get_llm(self):
        """LangChain LLM ê°ì²´ ë°˜í™˜"""
        return self.llm


class LlamaCppClient(LLM):
    """ì™¸ë¶€ llama-cpp API í´ë¼ì´ì–¸íŠ¸ (ë¡œì»¬ ëª¨ë¸ìš©)"""
    
    api_url: str = Field(default_factory=lambda: os.getenv("LLAMA_CPP_API_URL", "http://localhost:8000"))
    api_key: Optional[str] = Field(default_factory=lambda: os.getenv("LLAMA_CPP_API_KEY"))
    model_name: str = Field(default_factory=lambda: os.getenv("LLM_MODEL_NAME", "default"))
    temperature: float = Field(default_factory=lambda: float(os.getenv("LLM_TEMPERATURE", "0.0")))
    max_tokens: int = Field(default_factory=lambda: int(os.getenv("LLM_MAX_TOKENS", "2048")))
    timeout: int = Field(default_factory=lambda: int(os.getenv("LLM_TIMEOUT", "120")))
    
    @property
    def _llm_type(self) -> str:
        return "llama-cpp"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager:  Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """LLM í˜¸ì¶œ"""
        import httpx
        
        headers = {"Content-Type": "application/json"}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        
        payload = {
            "prompt": prompt,
            "temperature": kwargs.get("temperature", self.temperature),
            "max_tokens":  kwargs.get("max_tokens", self.max_tokens),
            "stop": stop or [],
        }
        
        try:
            with httpx.Client(timeout=self.timeout) as client:
                response = client. post(
                    f"{self.api_url}/v1/completions",
                    json=payload,
                    headers=headers
                )
                response.raise_for_status()
                result = response.json()
                
                if "choices" in result and len(result["choices"]) > 0:
                    return result["choices"][0]["text"]. strip()
                else:
                    return result. get("content", "").strip()
                    
        except Exception as e:
            raise Exception(f"llama-cpp API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    @property
    def _identifying_params(self) -> Dict[str, Any]:
        return {
            "api_url": self.api_url,
            "model_name": self.model_name,
            "temperature": self. temperature,
            "max_tokens": self.max_tokens,
        }


def get_llm(use_local:  bool = None):
    """
    LLM ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    
    Args:
        use_local: Trueë©´ llama-cpp ì‚¬ìš©, Falseë©´ Gemini ì‚¬ìš©
                   Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ USE_LOCAL_LLM í™•ì¸
    """
    if use_local is None:
        use_local = os.getenv("USE_LOCAL_LLM", "false").lower() == "true"
    
    if use_local:
        print("ğŸ¦™ ë¡œì»¬ llama-cpp ëª¨ë¸ ì‚¬ìš©")
        return LlamaCppClient()
    else:
        print("âœ¨ Google Gemini API ì‚¬ìš©")
        client = GeminiClient()
        return client.get_llm()
